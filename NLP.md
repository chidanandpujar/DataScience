\========================================================================  
## NLP \- Natural Language Processing  
\========================================================================

The introduction to NLP  
The need for NLP  
Applications of NLP  
Tasks in NLP  
Challenges in NLP  
Text Preprocessing 

\========================================================================  
Introduction :  
\========================================================================

We human beings exchange information via reading, writing, watching, speaking, listening etc.

This information exchange happens every day and everywhere in our day-to-day lives   
Some of the examples \- Meetings and catch-up calls, Reading news, Prescription from a doctor,  
exchanging ideas over social media.

During this information exchange , an enormous amount of data through natural language   
(in either written or spoken form) is being generated by an individual or an organization.

How many tweets are sent per day 2023?  
500 million  
Twitter users in the US, on average, spend 34.1 minutes on the platform daily.   
On average, 6000 tweets are sent every second, that is 500 million a day   
and 200 billion tweets are sent out annually.7 Nov 2023\.

This unstructured data is hence a potential gold mine and if converted into a meaningful form,   
it can be analyzed and valuable insights can be procured, which in turn can facilitate   
informed data driven decision-making.  
	  
What is NLP ?  
Natural language processing is a branch of Artificial intelligence, that deals with   
the interaction between machines and human in natural languages.

Natural Language Processing (NLP) is a field of Artificial Intelligence (AI)   
and Computer Science that is concerned with the interactions between computers   
and humans in natural language. The goal of NLP is to develop algorithms and   
models that enable computers to understand, interpret, generate, and   
manipulate human languages.

objective of NLP is to automate the reading , interpretation and   
understanding of human languages is also called as natural language understanding

Humans \------Natural Language via text/speech-------- Humans

Humans \------NLP------- Machines/Bots

NLP deals with how to make machine understand, read, interpret and generate   
the text or speech. NLP deals with taking the corpus and pre-process it and   
feed into the ML/DL models, so that machine is able to understand the text   
or speech and generate/predict the new text.

Two main parts of NLP are  
Natural Language Understanding \- NLU  
Natural Language Generation \- NLG

Natural Language Understanding (NLU) ‚Ää‚Äî‚ÄäThe computer‚Äôs ability to understand   
what we say.

Natural Language Generation‚Ää (NLG) ‚Äî‚ÄäGenerative AI. The generation of   
natural language by a computer, which predicts or generates the text. 

For example chatGPT 

\========================================================================  
NLP \- The need for NLP :  
\========================================================================  
analyze the unstructured data 

Text data is different because it cannot directly be input into machine learning and   
deep learning models like other numerical forms of data, Text data requires series of   
preprocessing steps (NLP Pipeline) before it can be analyzed and mined for insights.

Text data needs to be processed before we feed into ML/DL algorithms.

Preprocessing steps called as NLP pipeline,   
In comparison to general machine learning pipelines, In NLP we need to perform some extra processing steps.   
The reason is very simple that machines don‚Äôt understand the text. Here our biggest problem is How to make the text understandable for machines.  
Reference \- https://www.geeksforgeeks.org/natural-language-processing-nlp-pipeline/

The data is sequential in nature , changing or reversing the order of words/sentences changes its meaning.

This should give a sense of the need for a unique class of models capable of making predictions on text data  
and this is what establishes the need for NLP as a separate domain within Data Science and Artificial Intelligence 

\========================================================================  
Applications of NLP  
\========================================================================

Search Engines  \- Auto suggest / Auto complete and Auto correction of the sentences    
For Example google and Bing search engines 

Autocomplete \- suggestions for possible search keywords after you type a few characters.

Autocorrect \- Automatic spelling correction.

Automatic Email filtering \- Emails are automatically assigned to a category like primary,   
social, spam.

Text classification  \- Identify the patterns in the messages and classifying it.

Language Translation \- Translation from one language to another language    
For example Google translate \- https://translate.google.co.in/?hl=en\&tab=TT

Optical Character recognition \- converting images of hand written, typed or printed text into  
machine-encoded language.  
For example like converting physical checks, notebooks, textbooks to digital data.

Voice Assistants  is software which understands humans spoken requests and performs actions   
based on speech recognition, natural language comprehension and natural language processing.  
For example Amazon Alexa , Apples Siri 

\========================================================================  
Tasks in NLP  
\========================================================================

Text classification aims to automatically determine the class or category to which the piece of text belongs,  
applications like sentiment analysis, spam vs ham detection, topic labeling etc.

Text generation software is able to generate text and audio using ML/DL algorithms   
For Example Gmail is now able to suggest entire sentences based on previous sentences  
you have drafted .

Text Summarization takes an input of sequence of words called input article and returns the output   
words called summary.   
Text summarization can be a useful case study in domains like financial research, question-answer bots,   
media monitoring, social media marketing.

Text translation \- translating from source language to target language   
For example Google translate \- https://translate.google.co.in/?hl=en\&tab=TT

Text to speech processing and vice versa.

BOT developement where humans speak to trained systems and get a suitable response for their queries.

\========================================================================  
Challenges in NLP  
\========================================================================

Ambiguity is the main challenge of natural language processing because in natural language,   
words are unique, but they have different meanings depending upon the context which causes ambiguity.

For example :   
Apple unveils iPhone 15 Pro and iPhone 15 Pro Max

An apple a day keeps the doctor away

\========================================================================  
Text Preprocessing   
\========================================================================  
Text preprocessing deals with cleaning raw and Uncleaned text using following methods  
Lower casing  
Removal of Punctuations  
Removal of Stopwords  
Removal of Frequent words  
Removal of Rare words  
Stemming  
Lemmatization  
Removal of emojis  
Removal of emoticons  
Conversion of emoticons to words  
Conversion of emojis to words  
Removal of URLs  
Removal of HTML tags  
Spelling correction

Reference : https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing

\========================================================================  
NLP Pipeline  
\========================================================================  
Most important steps of NLP pipeline are as follows 

Data Acquisition

Text Preprocessing 

Representation  
or Text Representation   
or Text Vectorization   
or Feature Engineering           

Modeling  
or Model Building       

Deployment

\========================================================================  
Introduction to Text preprocessing  
\========================================================================  
Input is text data   
Text data coming from various sources is not entirely clean and it is unstructured.

If we take the data from websites, it contains html tags and from NLP perspective, this data is raw and uncleaned data.

For example :  
\<\!DOCTYPE html\>  
\<html\>  
\<body\>

\<h1\>My First Heading\</h1\>  
\<p\>My first paragraph.\</p\>

\</body\>  
\</html\>	

The process of transforming the raw and uncleaned text data into a form that is analyzable   
for the model is known as Text preprocessing.

If we train the model on unstructured data that hasn't been preprocessed , the model can   
miss out on learning important information.

Text data from social media platforms  like twitter, whatsapp, Facebook, instagram contain  
punctuations, special characters/symbols, emojis, emoticons, misspellings, urls/html tags,   
xml tags, accented letters, stopwords, hashtags, special characters, upper case and lower case letters.

Exact Nature of Text preprocessing also differs from task to task.  
For Example \- Tasks like grammar checks, text generation might need the stopwords to understand  
the complete meaning of the sentence.  
Tasks like Text summarization, text classification might not need to the stopwords, so removal of  
the stops words is necessary step.

\========================================================================  
Treating Accented   
\========================================================================  
In Text preprocessing we usually remove accented characters, because our ML/DL models consider  
words with and without an accent symbols as separate words, even though they may be the same word.

For example  
Hope you are having a good week. Just checking in √±√≥ √±√≥

text \= \[\]

for index in range(data.shape\[0\]):  
  sentence \= data\['text'\]\[index\].split()  
  new\_text \= \[unidecode.unidecode(word) for word in sentence\]  
  new\_text \= ' '.join(new\_text)  
  text.append(new\_text)

data\['accented\_unidecode'\] \= text  
data.loc\[0:3, \['text', 'accented\_unidecode'\]\]

\========================================================================  
Lower casing  
\========================================================================  
Same word written in two different cases, gives the model redundant information.  
Lowercasing converts all the words into lowercase to ensure that repeated occurrences of the same word in   
different cases are still treated as the same word .

df\["text\_lower"\] \= df\["text"\].str.lower()  
df.head()

\========================================================================  
Removal of Special characters or punctuations  
\========================================================================  
A special character is a character that is not an alphabetic or numeric character.  
For example \!"\#$%&'()\*+,-./:;\<=\>?@\[\\\]^\_\`{|}\~ etc 

These special characters or punctuations add the noise in unstructured text, and add no value to the meaning of the text,  
so removing them is preferable.

def remove\_special\_chars(text):  
  words \= text.split()  
  new\_text \= ‚Äú ‚Äò.join(\[ w for w in words if w.isalnum()\])  
  return new\_text

df\[‚Äúremo\_special\_chars‚Äù\]  \= df\[‚Äútext‚Äù\].apply(lambda x : remove\_special\_chars(x))

\========================================================================  
stopwords   
\========================================================================  
Example :   
Hey\!, Excellent movie, 1st half of movie is exceptional 2nd half bit more lagging and more   
violence apart from that super movie. Don't go with critics they are just useless.   
Enjoy movie in movie way, the movie is not bad  :-)

In the above example words like "of is and that they are in" etc are not adding any new information,  
such words are referred to as Stop words.

The idea behind removing stop words is that by eliminating low information parts of the text, we can   
concentrate on the key words.

This can be very specific to the NLP task that we are performing.

Removing of stopwords can even alter the meaning of sentence, so to avoid such kind of issues, it is   
advisable to choose the stopwords manually according to the NLP task.

\#Removal stopwords  
import nltk  
nltk.download('stopwords')  
from nltk.corpus import stopwords  
STOPWORDS \= set(stopwords.words('english'))  
print("List of stopwords {}".format(STOPWORDS))  
def remove\_stopwords(text):  
  return " ".join(\[word for word in str(text).split() if word not in STOPWORDS\])  
df\['headline'\] \= df\['headline'\].apply(lambda text: remove\_stopwords(text))  
df.head()

\========================================================================  
Removal of urls  
\========================================================================  
def remove\_urls(text):  
    url\_pattern \= re.compile(r'https?://\\S+|www\\.\\S+')  
    return url\_pattern.sub(r'', text)

df\['headline'\] \= df\['headline'\].apply(lambda text: remove\_urls(text))  
df.head()

\========================================================================  
Removal of html  
\========================================================================  
def remove\_html(text):  
    html\_pattern \= re.compile('\<.\*?\>')  
    return html\_pattern.sub(r'', text)

df\['headline'\] \= df\['headline'\].apply(lambda text: remove\_html(text))  
df.head()

\========================================================================  
Remove numbers  
\========================================================================

def remove\_numbers(text):  
  text \= re.sub(r‚Äô\\d+‚Äô, ‚Äò ‚Äò, text)  
  return text

data\[‚Äútext‚Äù\] \= data\[‚Äútext‚Äù\].apply(lambda x: remove\_numbers(x))  
data\[‚Äúsummary‚Äù\] \= data\[‚Äúsummary‚Äù\].apply(lambda x: remove\_numbers(x))

\========================================================================  
Remove ASCII characters  
\========================================================================

def remove\_non\_ascii(words):  
  ‚Äú‚Äù‚Äù Remove no-ASCII characters from list of tokenized words ‚Äú‚Äù‚Äù  
  new\_words \= \[\]  
  for word in words:  
    new\_word \= unicodedata.normalize(‚ÄòNFKD‚Äô, word).encode(‚Äòascii‚Äô, ‚Äòignore‚Äô).decode(‚Äòutf-8‚Äô, ‚Äòignore‚Äô)  
    new\_words.append(new\_word)  
  return new\_words

\========================================================================  
Convert to lower case  
\========================================================================

Def to\_lowercase(words):  
  ‚Äú‚Äù‚Äù‚Äù Convert all characters to lowercase from list of tokenized words ‚Äú‚Äù‚Äù‚Äù  
  new\_words \= \[\]  
  for word in words:  
    new\_word \= word.lower()  
    new\_words.append(new\_word)  
  Return new\_words  
  

\========================================================================  
Remove punctuations  
\========================================================================  
def remove\_punctuation(words):  
  ‚Äú‚Äù‚ÄùRemove punctuations from list of tokenized words‚Äù‚Äù‚Äù  
  new\_words \= \[\]  
  For word in words:  
    New\_word \= re.sub(r‚Äô\\w\\s‚Äô, ‚Äò ‚Äò, word)  
    If new\_word \!= ‚Äò ‚Äò:  
      news\_append.append(new\_word)  
  return new\_words

\========================================================================Removing extra spaces  
\========================================================================  
This helps remove spaces at the beginning and the end of the string/sentences

text \= \[\]  
for i in range(data.shape\[0\]):  
  result \= data\['text'\]\[i\].strip()  
  text.append(result)

data\['extra\_spaces'\] \= text  
data.head()

\========================================================================  
Tokenization   
\========================================================================  
In this step, we break a stream of raw text into small chunks of words or sentences known as tokens.

There are two types of tokenization  
1\) Word tokenization  
2\) Sentence tokenization

1\) Word Tokenization 

A sentence can break into individual words based on a specific delimiter like space.

An apple a day. Keeps the doctor away.

8 tokens 

An  
apple  
a  
day  
keeps  
the  
doctor  
away 

Dis advantage of word tokenization is that it cannot handle out-of Vocabulary words.

2\) Sentence Tokenization  
Same sentence can be break into multiple sentences base on different delimiters like "." a period

2 tokens

An apple a day  
Keeps the doctor away

\========================================================================  
Stemming  
\========================================================================  
While communicating we add suffix or prefix to the words, this is called as inflection.  
For example Teacher, Teaches, Teaching.

Presence of the such inflections in the text causes redundancy.

Converting the inflections of words to the base form or the stem word is called stemming.

ML/DL models considers these as different words, and this adds more dimensions to the trainig data.

For example  
\[Inflections\] \==============\> \[Stem\] Like  
Likes  
Liked  
Liking  
Likely 

Stemming is faster and it is based on rule-based approach

Different types of stemmer are as follows

Porter stemmer  
Snowball stemmer  
Lancaster stemmer

Porter stemmer  
Martin Porter created the Porter Stemmer algorithm  
It only works on the English Language  
Main advantage is its speed, simplicity, and ability to generate high quality outputs  
It uses rule-based approach, removes suffixes

Word           Suffix		Stem

Teacher        er		Teacher	  
Teaches   \=\>   es		Teach  
Teaching       ing		Teach

Coder          \-		Coder  
Coding         ing		Code  
codes          			code

Snowball stemmer  
https://www.nltk.org/api/nltk.stem.snowball.html?highlight=stemmer\#nltk.stem.snowball.EnglishStemmer  
Martin Porter created the Snowball stemmer, it is an improved version of Porter stemmer  
It is referred to as Porter2 stemmer  
Supports multiple languages, not just English

Word		Suffix		Stem  
Teacher		er		Teacher  
Teaches		es		Teach  
Teaching	in		Teach

Coder		\-		Coder  
Coding		ing		Code  
Codes		s		Code

Lancaster stemmer  
It is more aggressive stemmer, which means its output is sometimes aggressively stemmed, it is called as over stemming.  
Resulted words may or may not have any linguistic meaning

Word		Suffix		Stem

Teacher		er		Teach  
Teaches		es		Teach  
Teaching	ing		Teach

Coder		er		Cod  
Coding		ing		Cod  
Codes		es		Cod

Advantages  
Can process words more quickly than Lemmatization

Limitations of stemming

Overstemming  
understemming  
It simply removes prefix or suffix.  
Output always has no dictionary meaning.  
Accuracy is lower

\========================================================================  
Lemmatization  
\========================================================================  
Lemmatization reduces the word inflections to their dictionary form, referred to as Lemma  
Lemma of a word returned always has a dictionary meaning, it is also known as dictionary-based approach.

Word		Lemma

Teaches		Teaches  
Teaching	Teaching  
Teacher		Teacher

Coder		Coder  
Codes		Code  
Coding		Coding

Advantages  
It uses dictionary-based approach and output words always has dictionary meaning

Limitations   
Processing is slower compared to Stemming  
Accuracy is higher 

\========================================================================  
\# Removal of Emojis  
\========================================================================  
def remove\_emoji(string):  
    emoji \= re.compile("\["  
                           u"\\U0001F600-\\U0001FFFF"  \# emoticons  
                           u"\\U0001F300-\\U0001F5FF"  \# symbols & pictographs  
                           u"\\U0001F680-\\U0001F6FF"  \# transport & map symbols  
                           u"\\U0001F1E0-\\U0001F1FF"  \# flags (iOS)  
                           u"\\U00002702-\\U000027B0"  
                           u"\\U000024C2-\\U0001F251"  
                           "\]+", flags=re.UNICODE)  
    return emoji.sub(r'', string)

df\['headline'\] \= df\['headline'\].apply(lambda text: remove\_emoji(text))  
df.head()

\========================================================================  
Removal of emoticons  
\========================================================================  
def remove\_emoticons(text):  
    emoticon\_pattern \= re.compile(u'(' \+ u'|'.join(k for k in EMOTICONS) \+ u')')  
    return emoticon\_pattern.sub(r'', text)

remove\_emoticons("Hello :-)")

\========================================================================  
Convert emoticons  
\========================================================================  
def convert\_emoticons(text):  
    for emot in EMOTICONS:  
        text \= re.sub(u'('+emot+')', "\_".join(EMOTICONS\[emot\].replace(",","").split()), text)  
    return text

text \= "Hello :-) :-)"  
convert\_emoticons(text)

\========================================================================  
Convert emojis  
\========================================================================  
def convert\_emojis(text):  
    for emot in UNICODE\_EMO:  
        text \= re.sub(r'('+emot+')', "\_".join(UNICODE\_EMO\[emot\].replace(",","").replace(":","").split()), text)  
    return text

text \= "game is on üî•"  
convert\_emojis(text)

\========================================================================  
Spelling Correction  
\========================================================================

\!pip install autocorrect  
from autocorrect import Speller  
spell \= Speller()  
def autospell(text):  
  word \= text.split()  
  spells \= \[ spell(w) for w in word\]  
  return " ".join(spells)

text \= \[\]  
for i in range(data.shape\[0\]):  
  result \= autospell(data\['text'\]\[i\])  
  text.append(result)

data\['spell\_correct'\] \= text

\========================================================================  
Contractions   
Decontraction of words   
Ex:  
I‚Äô ll 	\-\>	I will  
He‚Äô ll	\-\>	He will  
I‚Äôm	\-\>	I am  
Can‚Äôt	\-\>	Can not  
Won‚Äôt	\-\>	will not  
Aren‚Äôt	\-\>	are not  
Doesn‚Äôt \-\>	Does not  
Haven‚Äôt \-\>	Have not

S \= ‚ÄúI‚Äôll, he‚Äôll, I‚Äôm, can‚Äôt, won‚Äôt, aren‚Äôt, doesn‚Äôt, haven‚Äôt‚Äù\]  
\[ connections.fix(w) for w in S.split()\]

\========================================================================  
Removal of html tags  
\========================================================================

def strip\_html(text):  
  soup \= BeautifulSoup(text, ‚Äúhtml.parser‚Äù)  
  Return soup.get\_text()

data\[‚Äòtext‚Äô\] \= data\[‚Äútext‚Äù\].apply(lambda x: strip\_html(x))  
data\[‚ÄúSummary‚Äù\] \= data\[‚ÄúSummary‚Äù\].apply(lambda x: strip\_html(x))

\========================================================================  
POS Tagging \- Part of speech tagging   
\========================================================================

Part-of-Speech (POS) tagging is a preprocessing step in natural language processing (NLP) that involves assigning a grammatical category or part-of-speech label (such as noun, verb, adjective, etc.) to each word in a sentence.  
For example The cat sat on the mat  
The \- Determiner  
cat \- Noun  
sat \- Verb  
on \- preposition  
the \- determiner  
mat \- noun

\========================================================================  
NLTK \- Natural Language ToolKit  
\========================================================================  
NLTK is the one of the most widely used NLP library  
Many of the functionalities provided by NLTK are currently limited to English only.  
Most of the text preprocessing task can be performed using NLTK.  
Tokenization  
Stemming  
Stopword removal  
POS tagging etc

NLTK provides support for Natural Language Generation tasks like Translation and Chatbots

NLTK package requires more storage space, but it is less memory intensive

\========================================================================  
spaCy  
\========================================================================  
spaCy is library for Natural Language Processing tasks  
spaCy‚Äôs Statistical Models  
spaCy‚Äôs Processing Pipeline

spaCy‚Äôs Statistical Models  
These models are the power engines of spaCy. These models enable spaCy to perform several NLP related tasks, such as part-of-speech tagging, named entity recognition, and dependency parsing.

Below are the different statistical models in spaCy along with their specifications:

en\_core\_web\_sm: small  
en\_core\_web\_md: medium  
en\_core\_web\_lg: large 

Importing these models is super easy. We can import a model by just executing spacy.load(‚Äòmodel\_name‚Äô) as shown below:

import spacy  
nlp \= spacy.load('en\_core\_web\_sm')

spaCy‚Äôs Processing Pipeline  
The first step for a text string, when working with spaCy, is to pass it to an NLP object. This object is essentially a pipeline of several text pre-processing operations through which the input text string has to go through.

spacy pipeline

import spacy  
nlp \= spacy.load('en\_core\_web\_sm')

\# Create an nlp object  
doc \= nlp("The cat sat on the mat")  
nlp.pipe\_names

spaCy doest not support tasks in the domain of Natural Languge Generation

spaCy offers different package sizes (small, medium, and large) and is more memory intensive

Functionalities works for multiple languages apart from English Languge

\========================================================================  
Named Entity Recognition  
\========================================================================  
Named Entity Recognition, or NER for short, is a subtask of NLP that focuses on identifying and classifying entities within textual data. These entities encompass a diverse range of information, including names of individuals, organizations, locations, dates, numerical values, and more.

Some of the categories that are the most important architecture in NER such that:  
Person  
Organization  
Place/ location  
Other common tasks include classifying of the following:

date/time.  
expression  
Numeral measurement (money, percent, weight, etc)  
E-mail address

  (PERSON Coder/NNP)  
  (PERSON Teacher/NNP Teaches/NNP Teaching/NNP))  
\========================================================================  
N-grams  
\========================================================================  
Collocations are group of words occurring together many times in a document.

Unigram \- An N-gram consisting of a single item from a sequence   
Bigrams \- An N-gram consisting of a combination of two words from a sequence  
Trigrams \- An N-gram consisting of a combination of three words from a sequence

For Example  
Cat sat on the mat. Dog sat on the floor.

bigrams:  
\[('The', 'cat'),  
 ('cat', 'sat'),  
 ('sat', 'on'),  
 ('on', 'the'),  
 ('the', 'mat'),  
 ('mat', '.'),  
 ('.', 'The'),  
 ('The', 'dog'),  
 ('dog', 'sat'),  
 ('sat', 'on'),  
 ('on', 'the'),  
 ('the', 'floor'),  
 ('floor', '.'),  
 ('.', 'Coder'),  
 ('Coder', 'codes'),  
 ('codes', 'Coding'),  
 ('Coding', '.'),  
 ('.', 'Teacher'),  
 ('Teacher', 'Teaches'),  
 ('Teaches', 'Teaching')\]

Trigrams

\[('The', 'cat', 'sat'),  
 ('cat', 'sat', 'on'),  
 ('sat', 'on', 'the'),  
 ('on', 'the', 'mat'),  
 ('the', 'mat', '.'),  
 ('mat', '.', 'The'),  
 ('.', 'The', 'dog'),  
 ('The', 'dog', 'sat'),  
 ('dog', 'sat', 'on'),  
 ('sat', 'on', 'the'),  
 ('on', 'the', 'floor'),  
 ('the', 'floor', '.'),  
 ('floor', '.', 'Coder'),  
 ('.', 'Coder', 'codes'),  
 ('Coder', 'codes', 'Coding'),  
 ('codes', 'Coding', '.'),  
 ('Coding', '.', 'Teacher'),  
 ('.', 'Teacher', 'Teaches'),  
 ('Teacher', 'Teaches', 'Teaching')\]

\========================================================================  
Word Cloud  
\========================================================================  
A word cloud is a visual representation of text, in which the words appear bigger the more often they are present.  
word cloud helps on getting insights on trends and patterns 

all\_text \= " ".join(texts for texts in data.text)  
wordcloud \= WordCloud(max\_font\_size \= 40, max\_words=100).generate(all\_text)  
plt.figure(figsize=(8,12))  
plt.imshow(wordcloud)  
plt.axis("off")  
plt.show()

\========================================================================  
NLP Conceptual terminologies  
\========================================================================  
corpus or corpora \- a single document or collection of documents   
can be single language of texts, or can span multiple languages.  
For example All the articles of a news paper, Blogs grom blogging sites, Threads of a Discussion Form

document \- collection of words or text sequences , text body.  
For example An email, A news article, A movie review

word \- smallest unit of text 

Vocabulary refers to distinct words in a selected corpus  
For example   
	corpus \- cat sat on the mat. Dog sat on the floor  
        Vocabulary \- 'cat' 'sat' 'on' 'the' 'mat' 'Dog' 'floor'

Out-of-Vocabulary any word in a document which is not found in the relevant corpus vocabulary is considered as   
Out of vocabulary.

Word Sense Disambiguation The ability to computationally identify the meaning of words in context is known as Word Sense  
Disambiguation

An algorithm trying to determine whether a text reference to the word "apple" refers to the fruit or the brand  
An apple a day keeps doctor away 

Apple is the best smartphone brand

\========================================================================Word Embedding  
\========================================================================  
Word Embedding is the process of mapping a word to a unique numerical representation.  
This can be done in multiple ways, such as vectorization where each word is uniquely represented by a vector  
(array of values)

ML/DL models needs numerical representations of words 

\========================================================================Vectorization  
\========================================================================  
Machines can not really understand the text as input

In order to perform Machine Learning on text, we need to convert text into a numerical format that machines  
can understand in order to find patterns and make predictions

on-hot encoding  
For example  
The 	Queen 	has 	entered		the 	room  
1	0	0	0		0	0  
0	1	0	0		0	0  
0	0	1	0		0	0  
0	0	0	1		0	0  
0	0	0	0		1	0  
0	0	0	0		0	1

Every single word is converted into vector element  
Queen is \[ 0,1,0,0,0,0\]

Disadvantages  
sparse matrix majority of the elements are zeroes  
consumes lot of memory, when the size of the corpus increases  
Computation expensive 

To solve we use Vectorization

The process of converting text data into vector format can be referred to as Vectorization  
Vector is a data structure similar to an array  
Computers can easily process the vectors

Bag of words (BOW)  
Term Frequency \- Inverse Document Frequency (TF-IDF)

Vocabulary  
The set of unique words used in the corpus after pre-processing the given text data.

Size of vocabulary  
The number of unique words in the vocabulary

Bag of words:

It used to vectorize a document  
The number of elements in the vector is equal to size of the vocabulary

We store the count of the word occurrences in a given text data

corpus		The cat sat on the mat.

Vocabulary	The cat sat on mat

Word		The 	cat	sat	on	mat  
Count		2	1	1	1	1

Values of the vector represent how frequently each word appears in corpus

Disadvantages  
sparse vector  
variable length bow  
Meaning of the sentence is lost if the order of the words is changed.

Term Frequency \- Inverse Document Frequency ( TF-IDF):  
Measures how important a word is in a corpus.

Each value in the vector corresponds to each word is product of TF and IDF  
vocabulary		TF-IDF vector	  
Word1			value1		TF of word1 \* IDF of word1  
Word2			value2  
Word3			value3  
Word4			value4  
Word5			value5

Term Frequency (TF) : It is the ration of number of times the term appears in a document and the   
total number of terms in the document

Document  
cat sat on mat.				  
dog sat on floor

TF (sat)	 	\= 	Number of times "cat" appeared in the document/ Total number of words in document  
				1/10

Inverse Document Frequence (IDF): It is the log of the ratio of total number of documents and   
the number of documents where the word appears.

Document1			Document2			Document3  
cat sat on mat			cat and dog sat on mat		mat and floor  
dog sat on floor

Total number of documents 	\= 		3  
Documents containing word cat 	\=		2

IDF (cat)			\=		log(3/2)

	  
TF-IDF (cat)			\=	(1/10) \* log(3/2)	

IDF reduce the importance of terms that are common to a lot of documents

A keyword appears only in a small number of documents, it is deemed more relevant to the documents   
in which it appears 

TF-IDF attempts to give higher relevance scores to the words that occur in fewer documents within the corpus.

\========================================================================  
TextBlob  
\========================================================================  
Library for processing textual data

NLP tasks such as part-of-speech tagging, sentimental analysis are more

textblob.sentiment module contains two sentimental analysis implementations  
PatternAnalyzer and NaiveBayesAnalyzer

NaiveBayesAnalyzer 

\========================================================================  
Classification report and Confusion matrix  
\========================================================================

\#TN/ True Negative  Actual was Negative and predicted was Negative  
\#TP/ True Positive  Actual was Positive and predicted was positive  
\#FN/ False Negative Actual was Positive and predicted was Negative  
\#FP/ False Positive Actual was Negative and predicted was positive

\# Precision \- What percent of your predictions were correct ?  
\# Precision is the ability of a classifier not to label an instance positive that is actually negative .

\#Precision is Accuracy of positive predictions

\#Precision \= TP/(TP \+ FP)

\# Recall \- what percent of the positive cases did you catch ?  
\# Recall is the ability of a classifier to find all positive instances 

\# Fraction of positives that were correctly identified 

\#Recall \= TP/ (TP \+ FN)

\# F1 score \- what percentage of positive predictions were correct ?  
\# Best score 1.0  
\# Worst score 0.0

\#F1 score= 2\* (Recall \* Precision)/ (Recall \+ Precision)

              precision    recall  f1-score   support

           0       1.00      0.86      0.92       936  
           1       0.00      0.00      0.00         0

    accuracy                           0.86       936  
   macro avg       0.50      0.43      0.46       936  
weighted avg       1.00      0.86      0.92       936

\# Model predicted every test message as ham (encoded as 0\)  
\# Model failed to predict test messages as spam ( encode as 1 )  
\# Total 936 messages of test data set are classified as ham  class .  
\# Model fails to identify any spam messages out of 936 messages of test data set.

y\_pred

array(\[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\])

y\_actual

array(\[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,  
       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,  
       1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,  
       0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,  
       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,  
       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,  
       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,  
       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,  
       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,  
       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,  
       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,  
       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  
       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,  
       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,  
       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,  
       0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,  
       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0\])

\========================================================================  
Reference   
\========================================================================

Bow \- unique set of words in corpus 

statistical techniques 

Binary Vector  
Count Vector  
TF-IDF   
co-occurent matrix 

1\) I love to read book

2\) I have to book a flight

window length 

Bag of words 

I 

Love

read

Book

to 

have 

a 

flight

TF-IDF 

word2vec

\===========

word featurization

1\) hope   can   you   set    free    len(BOW)  \= 5   
    0     1      2     3      4

one hot vectors using word index  
    1  
    0  
    0  
    0  
    0

dis advantages

solution is to convert dense vector 

Deep Learning \-  Neural Networks \- \> Neuron 

Role of Hidden Layers \- is featurization 

take the bow to target word \- hope and context words are remaining 

CBOW \- continuous Bag of words \- predicting target from context words 

skipgrow \- predicting context words from target word 

GLove vector

log(Xij) \- co occurrence of target and context words

Embedding 

word  to Feature Vector is  word embedding

Document to Feature Vector is Document embedding 

Sequential model \- 

semantic search 

NLP is hard 

Every time new words gets added 

words have different meaning based on the context  \- Apple  apple 

POS \- Part Of Speech tag   
classification of words to N/PN/Adj/Verb 

Language MOdel  \- Probabilistic distribution of words over a sentence 

unigram   
bigram   
trigram   
n-gram   
\========================================================================

NLP Terminologies

Text corpus or corpora/Corpus \- A corpus is a large set of text data that can be in one of the languages like English, French and so on.  
The corpus can consist of a single document or a bunch of documents

For ex \- 

Paragraph \- A paragraph is the largest unit of text handled by an NLP task . A set of sentences.

Sentences \- Combination of words.

phrases or words \- consecutive words or sequential words \- how are you 

N-grams \- Number of words combined together , single word  \- unigram, two words \- bi-gram  3 words \- tri-gram etc .

Input data \-\> NLP processing pipeline \-Text preprocessing \-\> Feature Engineering/Feature Extraction/Feature transformation \-\> output

NLP processing pipeline

NLP pre processing steps :

Text preprocessing   
Feature extraction/Feature Engineering \- convert to features in ML/DL understandable format like word2vev, TF-IDF,   
output 

1\. Input data \- Raw documents/tweets/customer reviews

2\. Pre-processing steps/Feature transformers

Tokenization  
Remove stopwords/punctuations/urls/lower casing etc  
stemming/Lemmatization  
Normalisation

Pre-processed tweets  
3\. Feature Extractor TF-IDF feature vectors  
4\. ML/DL models for classification

Popular NLP Libraries

NLTK \- the Natural Language Toolkit

spaCy \- 

TextBlob \- 

RASA \- 

Hugging Face \- 

\========================================================================  
Chatbot  
\========================================================================

Introduction to chatbots

what are chatbots ?  
Chatbots are simulations which can understand human language , process it   
and interact back with humans while performing specific tasks

The first chatbot was created by Joseph Wiesenbaum in 1966, name Eliza

Types of chatbots

Important types

Text-based chatbots

Voice-based chatbots

Chatbots are designed using these approaches

Rule-based chatbot: Bot answers questions based on some rules on which it is trained.  
The rules defined can be very simple to very complex

Self-learning chatbot: Bot that learns how to communicate using the result of machine learning  
model to learn and assess the current situation.

Top applications of chatbots  
Helpdesk assistant  
Home assistant  
Email distributor  
operations assistant  
phone assistant  
Entertainment assistant

Architecture of chatbots

Chat window or session\<-\>	Interface\<-\>	NLP model \<-\> Corpus  or Application DB

How does a chatbot work ?

1\. Import corpus  
2\. Preprocess the data  
3\. Text case handling  
4\. Tokenization  
5\. Stemming/Lemmatization  
6\. Bag of Words  
7\. One hot Embedding

Corpus: Corpus is the training data needed for the chatbot to learn

Without corpus, it is impossible for a chatbot to learn and reply   
something useful back to the user

Data preprocessing \- text case handling:  
Convert all the data coming as a input to either upper or lower case

Since Python is case sensitive , considers the same word in Upper and lower case   
as two different words and leads to misinterpretation of the words.

Tokenization:  
   
Word tokenization  
sentence tokenization 

word tokenization is converting a sentence into to individual collection of words  
Sentence tokenization is converting a paragraph in to individual collection of sentences .

This is a blog \-\> This  is  a  blog

Stemming and Lemmatization

Stemming is a process of finding the root words by removing suffix or prefixes .

Jump, Jumped, Jumps, Jumping \-\>	Jump

Bag of words(BOW):

This is a blog	\-\>	This	is	a	blog \-\>		Bag of Words

Process of converting words into numbers by generating vector embeddings from the tokens generated

	BOW Vector

	This	is	a	blog  
This 	1	0	0	0  
is	0	1	0	0  
a 	0	0	1	0  
blog	0	0	0	1

One hot encoding: One hot encoding is a process by which categorical variables are converted   
into a form that ML algorithms use		

Code a chatbot

\========================================================================  
Vectorization  
\========================================================================

Bag-of-Words (Count Vectorizer)

Bag of words converts text into set of vectors containing the count of word occurrences in the document

TF-IDF

TF-IDF creates vectors from text which contains information on the more important words and the less important ones as well

Word2Vec

Word2Vec creates vectors that are numerical representations of word features, features such as the context of individual words. The purpose and usefulness of Word2Vec is to group the vectors of similar words together in vector space. That is, it detects similarities mathematically.

\========================================================================

