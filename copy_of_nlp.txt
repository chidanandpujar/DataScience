
NLP - Natural Language Processing


The introduction to NLP
The need for NLP
Applications of NLP
Tasks in NLP
Challenges in NLP
Text Preprocessing 


Introduction :


We human beings exchange information via reading, writing, watching, speaking, listening etc.

This information exchange happens every day and everywhere in our day-to-day lives 
Some of the examples - Meetings and catch-up calls, Reading news, Prescription from a doctor,
exchanging ideas over social media.

During this information exchange , an enormous amount of data through natural language 
(in either written or spoken form) is being generated by an individual or an organisation.

How many tweets are sent per day 2023?
500 million
Twitter users in the US, on average, spend 34.1 minutes on the platform daily. 
On average, 6000 tweets are sent every second, that is 500 million a day 
and 200 billion tweets are sent out annually.7 Nov 2023.

This unstructured data is hence a potential gold mine and if converted into a meaningful form, 
it can be analyzed and valuable insights can be procured, which in turn can facilitate 
informed data driven decision-making.
	
What is NLP ?
Natural language processing is a branch of Artificial intelligence, that deals with 
the interaction between machines and human in naturual languages.

Natural Language Processing (NLP) is a field of Artificial Intelligence (AI) 
and Computer Science that is concerned with the interactions between computers 
and humans in natural language. The goal of NLP is to develop algorithms and 
models that enable computers to understand, interpret, generate, and 
manipulate human languages.

objective of NLP is to automate the reading , intepretation and 
understanding of human languages is also called as natual language understanding

Humans ------Natual Language via text/speech-------- Humans

Humans ------NLP------- Machines/Bots

NLP deals with how to make machine understand, read, interpret and generate 
the text or speech. NLP deals with taking the corpus and pre-process it and 
feed into the ML/DL models, so that machine is able to understand the text 
or speech and generate/predict the new text.

Two main parts of NLP are
Natural Language Understanding - NLU
Natural Language Generation - NLG

Natural Language Understanding (NLU)  — The computer’s ability to understand 
what we say.

Natural Language Generation  (NLG) — Generative AI. The generation of 
natural language by a computer, which predicts or generates the text. 

For example chatGPT 


NLP - The need for NLP :

anlayze the unstructured data 

Text data is different because it cannot directly be input into machine learning and 
deep learning models like other numerical forms of data, Text data requires series of 
preprocessing steps (NLP Pipeline) before it can be analyzed and mined for insights.

Text data needs to be processed before we feed into ML/DL algorithms.

Proprocessing steps called as NLP pipeline, 
In comparison to general machine learning pipelines, In NLP we need to perform some extra processing steps. 
The reason is very simple that machines don’t understand the text. Here our biggest problem is How to make the text understandable for machines.
Reference - https://www.geeksforgeeks.org/natural-language-processing-nlp-pipeline/

The data is sequential in nature , changing or reversing the order of words/sentences changes its meaning.

This should give a sense of the need for a unique class of models capable of making predictions on text data
and this is what establishes the need for NLP as a separate domain within Data Science and Artificial Intelligence 


Applications of NLP


Search Engines  - Auto suggest / Auto complete and Auto correction of the sentences  
For Example google and Bing search engines 

Autocomplete - suggestions for possible search keywords after you type a few characters.

Autocorrect - Automatic spelling correction.

Automatic Email filtering - Emails are automatically assigned to a category like primary, 
social, spam.

Text classification  - Identify the patterns in the messages and classifying it.

Language Translation - Translation from one language to another language  
For example Google translate - https://translate.google.co.in/?hl=en&tab=TT

Optical Character recognition - converting images of hand written, typed or printed text into
machine-encoded language.
For example like converting physical checks, notebooks, textbooks to digital data.

Voice Assistants  is software which undestands humans spoken requests and performs actions 
based on speech recognition, natural language comprehension and natural language processing.
For example Amazon Alexa , Apples Siri 


Tasks in NLP


Text classification aims to automatically determine the class or category to which the piece of text belongs,
applications like sentiment analysis, spam vs ham detection, topic labelling etc.

Text generation software is able to generate text and audio using ML/DL algorithms 
For Example Gmail is now able to suggest entire sentences based on previous sentences
you have drafted .

Text Summarization takes an input of sequence of words called input article and returns the output 
words called summary. 
Text summarization can be a useful case study in domains like financial research, question-answer bots, 
media monitoring, social media marketing.

Text translation - translating from source langauage to target language 
For example Google translate - https://translate.google.co.in/?hl=en&tab=TT

Text to speach processing and vice versa.

BOT developement where humans speak to trained systems and get a suitable response for their queries.



Challenges in NLP


Ambiguity is the main challenge of natural language processing because in natural language, 
words are unique, but they have different meanings depending upon the context which causes ambiguity.

For example : 
Apple unveils iPhone 15 Pro and iPhone 15 Pro Max

An apple a day keeps the doctor away



Text Preprocessing 

Text preprocessing deals with cleaning raw and Uncleaned text using following methods
Lower casing
Removal of Punctuations
Removal of Stopwords
Removal of Frequent words
Removal of Rare words
Stemming
Lemmatization
Removal of emojis
Removal of emoticons
Conversion of emoticons to words
Conversion of emojis to words
Removal of URLs
Removal of HTML tags
Spelling correction

Reference : https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing


NLP Pipeline

Most imortant steps of NLP pipeline are as follows 

Data Acquistion

Text Preprocessing 

Representation
or Text Representation 
or Text Vectorization 
or Feature Engineering           

Modelling
or Model Building       

Deployement


Introduction to Text preprocessing

Input is text data 
Text data coming from various sources is not entirely clean and it is unstructured.

If we take the data from websites, it contains html tags and from NLP perspective, this data is raw and uncleaned data.

For example :
<!DOCTYPE html>
<html>
<body>

<h1>My First Heading</h1>
<p>My first paragraph.</p>

</body>
</html>	

The process of transforming the raw and uncleaned text data into a form that is analyzable 
for the model is known as Text preprocessing.

If we train the model on unstructured data that hasn't been preprocessed , the model can 
miss out on learning important information.

Text data from social media platforms  like twitter, whatsapp, Facebook, instagram contain
punctuations, special characters/symbols, emojis, emoticons, misspellings, urls/html tags, 
xml tags, accented letters, stopwords, hashtags, special characters, upper case and lower case letters.

Exact Nature of Text preprocessing also differs from task to task.
For Example - Tasks like grammer checks, text generation might need the stopwords to understand
the complete meaning of the sentence.
Tasks like Text summerization, text classificat on might not need to the stopwords, so removal of
the stops words is necessary step.


Treating Accented 

In Text preprocessing we usually remove accented characters, because our ML/DL models consider
words with and without an accent symbols as separate words, even though they may be the same word.

For example
Hope you are having a good week. Just checking in ñó ñó

text = []

for index in range(data.shape[0]):
  sentence = data['text'][index].split()
  new_text = [unidecode.unidecode(word) for word in sentence]
  new_text = ' '.join(new_text)
  text.append(new_text)

data['accented_unidecode'] = text
data.loc[0:3, ['text', 'accented_unidecode']]


Lower casing

Same word written in two different cases, gives the model redundant information.
Lowercasing converts all the words into lowercase to ensure that repeated occurences of the same word in 
different cases are still treated as the same word .

df["text_lower"] = df["text"].str.lower()
df.head()


Removal of Special characters or punctuations

A special character is a character that is not an alphabetic or numeric character.
For example !"#$%&'()*+,-./:;<=>?@[\]^_`{|}~ etc 

These special characters or punctuations add the noise in unstructured text, and add no value to the meaning of the text,
so removing them is preferable.


stopwords 

Example : 
Hey!, Excellent movie, 1st half of movie is exceptional 2nd half bit more lagging and more 
violence apart from that super movie. Don't go with critics they are just useless. 
Enjoy movie in movie way, the movie is not bad  :-)

In the above example words like "of is and that they are in" etc are not adding any new information,
such words are referred to as Stop words.

The idea behind removing stop words is that by eliminating low information parts of the text, we can 
concentrate on the key words.

This can be very specific to the NLP task that we are performing.

Removing of stopwords can even alter the meaning of sentence, so to avoid such kind of issues, it is 
advisable to choose the stopwords manually according to the NLP task.

#Removal stopwords
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
STOPWORDS = set(stopwords.words('english'))
print("List of stopwords {}".format(STOPWORDS))
def remove_stopwords(text):
  return " ".join([word for word in str(text).split() if word not in STOPWORDS])
df['headline'] = df['headline'].apply(lambda text: remove_stopwords(text))
df.head()


Removal of urls

def remove_urls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', text)

df['headline'] = df['headline'].apply(lambda text: remove_urls(text))
df.head()



Removal of html

def remove_html(text):
    html_pattern = re.compile('<.*?>')
    return html_pattern.sub(r'', text)

df['headline'] = df['headline'].apply(lambda text: remove_html(text))
df.head()



Removing extra spaces

This helps remove spaces at the beginning and the end of the string/sentences

text = []
for i in range(data.shape[0]):
  result = data['text'][i].strip()
  text.append(result)

data['extra_spaces'] = text
data.head()


Tokenization 

In this step, we break a stream of raw text into small chunks of words or sentences known as tokens.

There are two types of tokenization
1) Word tokenization
2) Sentence tokenization

1) Word Tokenization 

A sentence can break into individual words based on a specific delimiter like space.

An apple a day. Keeps the doctor away.

8 tokens 

An
apple
a
day
keeps
the
doctor
away 

Dis advantage of word tokenization is that it cannot handle out-of Vocabulary words.

2) Sentence Tokenization
Same sentence can be break into multiple sentences base on different delimiters like "." a period

2 tokens

An apple a day
Keeps the doctor away


Stemming

While communicating we add suffix or prefix to the words, this is called as inflection.
For example Teacher, Teaches, Teaching.

Presence of the such inflections in the text causes redundancy.

Converting the inflections of words to the base form or the stem word is called stemming.

ML/DL models considers these as different words, and this adds more dimensions to the trainig data.

For example
[Inflections] ==============> [Stem] Like
Likes
Liked
Liking
Likely 

Stemming is faster and it is based on rule-based approach

Different types of stemmer are as follows

Porter stemmer
Snowball stemmer
Lancaster stemmer

Porter stemmer
Martin Porter created the Porter Stemmer algorithm
It only works on the English Language
Main advantage is its speed, simplicity, and ability to generate high quality outputs
It uses rule-based approach, removes suffixes

Word           Suffix		Stem

Teacher        er		Teacher	
Teaches   =>   es		Teach
Teaching       ing		Teach

Coder          -		Coder
Coding         ing		Code
codes          			code

Snowball stemmer
https://www.nltk.org/api/nltk.stem.snowball.html?highlight=stemmer#nltk.stem.snowball.EnglishStemmer
Martin Porter created the Snowball stemmer, it is an improved version of Porter stemmer
It is referred to as Porter2 stemmer
Supports multiple languages, not just English

Word		Suffix		Stem
Teacher		er		Teacher
Teaches		es		Teach
Teaching	in		Teach

Coder		-		Coder
Coding		ing		Code
Codes		s		Code

Lancaster stemmer
It is more aggressive stemmer, which means its output is sometimes aggressively stemmed, it is called as over stemming.
Resulted words may or may not have any linguistic meaning

Word		Suffix		Stem

Teacher		er		Teach
Teaches		es		Teach
Teaching	ing		Teach

Coder		er		Cod
Coding		ing		Cod
Codes		es		Cod

Advantages
Can process words more quickly than Lemmatization

Limitations of stemming

Overstemming
understemming
It simply removes prefix or suffix.
Output always has no dictionary meaning.
Accuracy is lower


Lemmatization

Lemmatization reduces the word inflections to their dictionary form, referred to as Lemma
Lemma of a word returned always has a dictionary meaning, it is also known as dictionary-based approach.

Word		Lemma

Teaches		Teaches
Teaching	Teaching
Teacher		Teacher

Coder		Coder
Codes		Code
Coding		Coding

Advantages
It uses dictionary-based approach and output words always has dictionary meaning

Limitations 
Processing is slower compared to Stemming
Accuracy is higher 


# Removal of Emojis

def remove_emoji(string):
    emoji = re.compile("["
                           u"\U0001F600-\U0001FFFF"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji.sub(r'', string)

df['headline'] = df['headline'].apply(lambda text: remove_emoji(text))
df.head()


Removal of emoticons

def remove_emoticons(text):
    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')
    return emoticon_pattern.sub(r'', text)

remove_emoticons("Hello :-)")


Convert emoticons

def convert_emoticons(text):
    for emot in EMOTICONS:
        text = re.sub(u'('+emot+')', "_".join(EMOTICONS[emot].replace(",","").split()), text)
    return text

text = "Hello :-) :-)"
convert_emoticons(text)


Convert emojis

def convert_emojis(text):
    for emot in UNICODE_EMO:
        text = re.sub(r'('+emot+')', "_".join(UNICODE_EMO[emot].replace(",","").replace(":","").split()), text)
    return text

text = "game is on 🔥"
convert_emojis(text)


Spelling Correction


!pip install autocorrect
from autocorrect import Speller
spell = Speller()
def autospell(text):
  word = text.split()
  spells = [ spell(w) for w in word]
  return " ".join(spells)

text = []
for i in range(data.shape[0]):
  result = autospell(data['text'][i])
  text.append(result)

data['spell_correct'] = text



POS Tagging - Part of speech tagging 


Part-of-Speech (POS) tagging is a preprocessing step in natural language processing (NLP) that involves assigning a grammatical category or part-of-speech label (such as noun, verb, adjective, etc.) to each word in a sentence.
For example The cat sat on the mat
The - Determiner
cat - Noun
sat - Verb
on - preposition
the - determiner
mat - nooun


NLTK - Natural Language ToolKit

NLTK is the one of the most widely used NLP library
Many of the functionalities provided by NLTK are currently limited to English only.
Most of the text preprocessing task can be performed using NLTK.
Tokenization
Stemming
Stopword removal
POS tagging etc

NLTK provides support for Natural Language Generation tasks like Translation and Chatbots

NLTK package requires more storage space, but it is less memory intensive

spaCy

spaCy is library for Natural Language Processing tasks
spaCy’s Statistical Models
spaCy’s Processing Pipeline

spaCy’s Statistical Models
These models are the power engines of spaCy. These models enable spaCy to perform several NLP related tasks, such as part-of-speech tagging, named entity recognition, and dependency parsing.

Below are the different statistical models in spaCy along with their specifications:

en_core_web_sm: small
en_core_web_md: medium
en_core_web_lg: large 

Importing these models is super easy. We can import a model by just executing spacy.load(‘model_name’) as shown below:

import spacy
nlp = spacy.load('en_core_web_sm')

spaCy’s Processing Pipeline
The first step for a text string, when working with spaCy, is to pass it to an NLP object. This object is essentially a pipeline of several text pre-processing operations through which the input text string has to go through.

spacy pipeline

import spacy
nlp = spacy.load('en_core_web_sm')

# Create an nlp object
doc = nlp("The cat sat on the mat")
nlp.pipe_names

spaCy doest not support tasks in the domain of Natural Languge Generation

spaCy offers different package sizes (small, medium, and large) and is more memory intensive

Functionalities works for multiple languages apart from English Languge


Named Entity Recognition

Named Entity Recognition, or NER for short, is a subtask of NLP that focuses on identifying and classifying entities within textual data. These entities encompass a diverse range of information, including names of individuals, organizations, locations, dates, numerical values, and more.

Some of the categories that are the most important architecture in NER such that:
Person
Organization
Place/ location
Other common tasks include classifying of the following:

date/time.
expression
Numeral measurement (money, percent, weight, etc)
E-mail address


  (PERSON Coder/NNP)
  (PERSON Teacher/NNP Teaches/NNP Teaching/NNP))

N-grams

Collocations are group of words occuring together many times in a document.

Unigram - An N-gram consisting of a single item from a sequence 
Bigrams - An N-gram consitiing of a combination of two words from a sequence
Trigrams - An N-gram consisting of a combination of three words from a sequence

For Example
Cat sat on the mat. Dog sat on the floor.

bigrams:
[('The', 'cat'),
 ('cat', 'sat'),
 ('sat', 'on'),
 ('on', 'the'),
 ('the', 'mat'),
 ('mat', '.'),
 ('.', 'The'),
 ('The', 'dog'),
 ('dog', 'sat'),
 ('sat', 'on'),
 ('on', 'the'),
 ('the', 'floor'),
 ('floor', '.'),
 ('.', 'Coder'),
 ('Coder', 'codes'),
 ('codes', 'Coding'),
 ('Coding', '.'),
 ('.', 'Teacher'),
 ('Teacher', 'Teaches'),
 ('Teaches', 'Teaching')]

Trigrams

[('The', 'cat', 'sat'),
 ('cat', 'sat', 'on'),
 ('sat', 'on', 'the'),
 ('on', 'the', 'mat'),
 ('the', 'mat', '.'),
 ('mat', '.', 'The'),
 ('.', 'The', 'dog'),
 ('The', 'dog', 'sat'),
 ('dog', 'sat', 'on'),
 ('sat', 'on', 'the'),
 ('on', 'the', 'floor'),
 ('the', 'floor', '.'),
 ('floor', '.', 'Coder'),
 ('.', 'Coder', 'codes'),
 ('Coder', 'codes', 'Coding'),
 ('codes', 'Coding', '.'),
 ('Coding', '.', 'Teacher'),
 ('.', 'Teacher', 'Teaches'),
 ('Teacher', 'Teaches', 'Teaching')]


Word Cloud

A word cloud is a visual representation of text, in which the words appear bigger the more often they are present.
word cloud helps on getting insights on trends and patterns 

all_text = " ".join(texts for texts in data.text)
wordcloud = WordCloud(max_font_size = 40, max_words=100).generate(all_text)
plt.figure(figsize=(8,12))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()


NLP Conceptual terminologies

corpus or corpora - a single document or collection of documents 
can be single language of texts, or can span multiple languages.
For example All the ariticles of a news paper, Blogs grom blogging sites, Threads of a Discussion Form

document - collection of words or text sequnces , text body.
For example An email, A news article, A movie review

word - smallest unit of text 

Vocabulary refers to distinct words in a selected corpus
For example 
	corpus - cat sat on the mat. Dog sat on the floor
        Vocabulary - 'cat' 'sat' 'on' 'the' 'mat' 'Dog' 'floor'

Out-of-Vocabulary any word in a document which is not found in the relevant corpus vocabulary is considered as 
Out of vocabulary.

Word Sense Disambiguation The ability to computationally identify the meaning of words in context is known as Word Sense
Disambiguation

An algorithm trying to determine whether a text reference to the word "apple" refers to the fruit or the brand
An apple a day keeps doctor away 

Apple is the best smartphone brand


Word Embedding

Word Embedding is the process of mapping a word to a unique numerical representation.
This can be done in multiple ways, such as vectorization where each word is uniquely represented by a vector
(array of values)

ML/DL models needs numerical representations of words 


Vectorization

Machines can not really understand the text as input

In order to perform Machine Learning on text, we need to convert text into a numerical format that machines
can understand in order to find patterns and make predictions

on-hot encoding
For example
The 	Queen 	has 	entered		the 	room
1	0	0	0		0	0
0	1	0	0		0	0
0	0	1	0		0	0
0	0	0	1		0	0
0	0	0	0		1	0
0	0	0	0		0	1

Every single word is converted into vector element
Queen is [ 0,1,0,0,0,0]

Dis advanatages
sparse matrix majority of the elements are zeroes
consumes lot of memory, when the size of the corpus increases
Computation expensive 

To solve we use Vectorization

The process of converting text data into vector format can be referred to as Vectorization
Vector is a data structure similar to an array
Computers can esily process the vectors

Bag of words (BOW)
Term Frequency - Inverse Document Frequency (TF-IDF)

Vocabulary
The set of unique words used in the corpus after pre-processing the given text data.

Size of vocabulary
The number of unique words in the vocabulary

Bag of words:

It used to vectorize a document
The number of elements in the vector is equal to size of the vocabulary

We store the count of the word occurence in a given text data

corpus		The cat sat on the mat.

Vocabulary	The cat sat on mat

Word		The 	cat	sat	on	mat
Count		2	1	1	1	1

Values of the vector represent how frequently each word appears in corpus

Dis advantages
sparse vector
variable length bow
Meaning of the sentence is lost if the order of the words is changed.

Term Frequency - Inverse Document Frequency ( TF-IDF):
Measures how important a word is in a corpus.

Each value in the vector corresponds to each word is product of TF and IDF
vocabulary		TF-IDF vector	
Word1			value1		TF of word1 * IDF of word1
Word2			value2
Word3			value3
Word4			value4
Word5			value5

Term Frequency (TF) : It is the ration of number of times the term appears in a document and the 
total number of terms in the document

Document
cat sat on mat.				
dog sat on floor

TF (sat)	 	= 	Number of times "cat" appeared in the document/ Total number of words in document
				1/10

Inverse Document Frequence (IDF): It is the log of the ratio of total number of documents and 
the number of documents where the word appears.

Document1			Document2			Document3
cat sat on mat			cat and dog sat on mat		mat and floor
dog sat on floor

Total number of documents 	= 		3
Documents conataining word cat 	=		2

IDF (cat)			=		log(3/2)

	
TF-IDF (cat)			=	(1/10) * log(3/2)	


IDF reduce the importance of terms that are common to a lot of documents

A keyword appears only in a small number of documents, it is deemed more relevant to the documents 
in which it appears 

TF-IDF attempts to give higher relevance scores to the words that occur in fewer documents within the corpus.


TextBlob

Library for processing textual data

NLP tasks such as part-of-speech tagging, sentimental analysis are more

textblob.sentiment module contains two sentimental analysis implementations
PatternAnalyzer and NaiveBayesAnalyzer

NaiveBayesAnalyzer 


Classification report and Confusion matrix


#TN/ True Negative  Actual was Negative and predicted was Negative
#TP/ True Positive  Actual was Positive and predicted was positive
#FN/ False Negative Actual was Postive and predicted was Negative
#FP/ False Positive Acutual was Negative and predicted was positive

# Precision - What percent of your predictions were correct ?
# Precision is the ability of a classifier not to label an instance positive that is actually negative .

#Precision is Accuracy of positive predictions

#Precision = TP/(TP + FP)


# Recall - what percent of the positive cases did you catch ?
# Recall is the ability of a classifier to find all positive instances 

# Fraction of positives that were correctly identfied 

#Recall = TP/ (TP + FN)

# F1 score - what percentage of positive predictions were correct ?
# Best socre 1.0
# Worst score 0.0

#F1 score= 2* (Recall * Precision)/ (Recall + Precision)


              precision    recall  f1-score   support

           0       1.00      0.86      0.92       936
           1       0.00      0.00      0.00         0

    accuracy                           0.86       936
   macro avg       0.50      0.43      0.46       936
weighted avg       1.00      0.86      0.92       936

# Model predicted every test message as ham (encoded as 0)
# Model failed to predict test messages as spam ( encode as 1 )
# Total 936 messages of test data set are classfied as ham  class .
# Model fails to identify any spam messages out of 936 messages of test data set.

y_pred

array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])


y_actual

array([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
       1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
       0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
       0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])


Bow - unique set of words in corpus 

statisitcal techniques 

Binary Vector
Count Vector
TF-IDF 
co-occurent matrix 


1) I love to read book

2) I have to book a flight

window length 


Bag of words 

I 

Love

read

Book

to 

have 

a 

flight



TF-IDF 



word2vec

===========

word featurization


1) hope   can   you   set    free    len(BOW)  = 5 
    0     1      2     3      4

one hot vectors using word index
    1
    0
    0
    0
    0

dis advantages


solution is to convert dense vector 

Deep Leaning -  Neural Networks - > Neuron 

Role of Hidden Layers - is featurization 

take the bow to target word - hope and context words are remaining 

CBOW - continous Bag of words - predicting target from context words 

skipgrow - predicting context words from target word 

GLove vector

log(Xij) - co occurence of target and context words


Embedding 

word  to Feature Vector is  word embedding

Document to Feature Vector is Document emdedding 

Sequential model - 

semantic search 

NLP is hard 

Every time new words gets added 

words have different meaning based on the context  - Apple  apple 

POS - Part Of Speech tag 
classification of words to N/PN/Adj/Verb 

Language MOdel  - Problablistic distribution of words over a sentence 

unigram 
bigram 
trigram 
n-gram 


NLP Terminologies

Text corpus or corpora/Corpus - A corpus is a large set of text data that can be in one of the languages like English, French and so on.
The corpus can consist of a single document or a bunch of documents

For ex - 


Paragraph - A paragraph is the largest unit of text handled by an NLP task . A set of sentences.

Sentences - Combination of words.

phrases or words - consecutive words or sequential words - how are you 

N-grams - Number of words combined together , single word  - unigram, two words - bi-gram  3 words - tri-gram etc .



Input data -> NLP processing pipeline -Text preprocessing -> Feature Engineering/Feature Extraction/Feature transformation -> output

NLP processing pipeline

NLP pre processing steps :

Text preprocessing 
Feature extraction/Feature Engineering - convert to features in ML/DL understandable format like word2vev, TF-IDF, 
output 

1. Input data - Raw documents/tweets/customer reviews

2. Pre-processing steps/Feature transformers

Tokenization
Remove stopwords/punctuations/urls/lower casing etc
stemming/Lemmatization
Normalisation

Pre-processed tweets
3. Feature Extractor TF-IDF feature vectors
4. ML/DL models for classification



Popular NLP Libraries

NLTK - the Natural Language Toolkit

spaCy - 

TextBlob - 

RASA - 

Hugging Face - 
















